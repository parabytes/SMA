{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Enterprise-setup\" data-toc-modified-id=\"Enterprise-setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Enterprise setup</a></span></li><li><span><a href=\"#Premium-Setup\" data-toc-modified-id=\"Premium-Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Premium Setup</a></span></li><li><span><a href=\"#Fast-Way\" data-toc-modified-id=\"Fast-Way-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Fast Way</a></span></li><li><span><a href=\"#Working-with-the-ResultStream\" data-toc-modified-id=\"Working-with-the-ResultStream-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Working with the ResultStream</a></span></li><li><span><a href=\"#Counts-Endpoint\" data-toc-modified-id=\"Counts-Endpoint-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Counts Endpoint</a></span></li><li><span><a href=\"#Dated-searches-/-Full-Archive-Search\" data-toc-modified-id=\"Dated-searches-/-Full-Archive-Search-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Dated searches / Full Archive Search</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with the API within a Python program is straightforward both for Premium and Enterprise clients.\n",
    "\n",
    "We'll assume that credentials are in the default location, `~/.twitter_keys.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from searchtweets import ResultStream, gen_rule_payload, load_credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enterprise setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cannot read file ../dependencies/twitter_keys.yaml\n",
      "Error parsing YAML file; searching for valid environment variables\n",
      "Account type is not specified and cannot be inferred.\n",
      "        Please check your credential file, arguments, or environment variables\n",
      "        for issues. The account type must be 'premium' or 'enterprise'.\n",
      "        \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f9899ccc4707>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menterprise_search_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_credentials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../dependencies/twitter_keys.yaml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myaml_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"search_tweets_enterprise\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_overwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/searchtweets/credentials.py\u001b[0m in \u001b[0;36mload_credentials\u001b[0;34m(filename, account_type, yaml_key, env_overwrite)\u001b[0m\n\u001b[1;32m    187\u001b[0m                    \u001b[0;32mif\u001b[0m \u001b[0menv_overwrite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                    else merge_dicts(env_vars, yaml_vars))\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0mparsed_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_credentials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccount_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccount_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/searchtweets/credentials.py\u001b[0m in \u001b[0;36m_parse_credentials\u001b[0;34m(search_creds, account_type)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \"\"\"\n\u001b[1;32m     81\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "enterprise_search_args = load_credentials(\"../dependencies/twitter_keys.yaml\", yaml_key=\"search_tweets_enterprise\", env_overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Premium Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premium_search_args = load_credentials(\"../dependencies/search_tweets_creds.yaml\",\n",
    "                                       yaml_key=\"search_tweets_30_day_dev\",\n",
    "                                       env_overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a function that formats search API rules into valid json queries called `gen_rule_payload`. It has sensible defaults, such as pulling more Tweets per call than the default 100 (but note that a sandbox environment can only have a max of 100 here, so if you get errors, please check this) not including dates, and defaulting to hourly counts when using the counts api. Discussing the finer points of generating search rules is out of scope for these examples; I encourage you to see the docs to learn the nuances within, but for now let's see what a rule looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_rule = input(\"Enter Boolean search \")\n",
    "rule = gen_rule_payload(input_rule, results_per_call=100) # testing with a sandbox account\n",
    "print(rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point, there are two ways to interact with the API. There is a quick method to collect smaller amounts of Tweets to memory that requires less thought and knowledge, and interaction with the `ResultStream` object which will be introduced later.\n",
    "\n",
    "\n",
    "## Fast Way\n",
    "\n",
    "We'll use the `search_args` variable to power the configuration point for the API. The object also takes a valid PowerTrack rule and has options to cutoff search when hitting limits on both number of Tweets and API calls.\n",
    "\n",
    "We'll be using the `collect_results` function, which has three parameters.\n",
    "\n",
    "- rule: a valid PowerTrack rule, referenced earlier\n",
    "- max_results: as the API handles pagination, it will stop collecting when we get to this number\n",
    "- result_stream_args: configuration args that we've already specified.\n",
    "\n",
    "\n",
    "For the remaining examples, please change the args to either premium or enterprise depending on your usage.\n",
    "\n",
    "Let's see how it goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from searchtweets import collect_results # to_date=\"202008210651"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = gen_rule_payload(\"#womaninstem OR #womenindatascience OR #womenintech\", results_per_call=100, to_date=\"202008180919\")\n",
    "print(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets = collect_results(rule,\n",
    "                         max_results=2000,\n",
    "                         result_stream_args=premium_search_args) # change this if you need to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Tweet payloads are lazily parsed into a `Tweet` [object](https://twitterdev.github.io/tweet_parser/). An overwhelming number of Tweet attributes are made available directly, as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "tweets_first_2000 = copy.deepcopy(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#with open('supernow.json', 'w') as outfile:\n",
    "#    json.dump(tweets, outfile)\n",
    "with open('202008180919.json', 'w') as outfile:\n",
    "    json.dump(tweets, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila, we have some Tweets. For interactive environments and other cases where you don't care about collecting your data in a single load or don't need to operate on the stream of Tweets or counts directly, I recommend using this convenience function.\n",
    "\n",
    "\n",
    "## Working with the ResultStream\n",
    "\n",
    "The ResultStream object will be powered by the `search_args`, and takes the rules and other configuration parameters, including a hard stop on number of pages to limit your API call usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = ResultStream(rule_payload=rule,\n",
    "                  max_results=500,\n",
    "                  max_pages=1,\n",
    "                  **premium_search_args)\n",
    "\n",
    "print(rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a function, `.stream`, that seamlessly handles requests and pagination for a given query. It returns a generator, and to grab our 500 Tweets that mention `beyonce` we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(rs.stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets are lazily parsed using our [Tweet Parser](https://twitterdev.github.io/tweet_parser/), so tweet data is very easily extractable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using unidecode to prevent emoji/accents printing \n",
    "[print(tweet.all_text) for tweet in tweets[0:10]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counts Endpoint\n",
    "\n",
    "We can also use the Search API Counts endpoint to get counts of Tweets that match our rule. Each request will return up to *30* results, and each count request can be done on a minutely, hourly, or daily basis. The underlying `ResultStream` object will handle converting your endpoint to the count endpoint, and you have to specify the `count_bucket` argument when making a rule to use it.\n",
    "\n",
    "The process is very similar to grabbing Tweets, but has some minor differences.\n",
    "\n",
    "\n",
    "_Caveat - premium sandbox environments do NOT have access to the Search API counts endpoint._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_rule = gen_rule_payload(\"beyonce\", count_bucket=\"day\")\n",
    "\n",
    "counts = collect_results(count_rule, result_stream_args=enterprise_search_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our results are pretty straightforward and can be rapidly used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dated searches / Full Archive Search\n",
    "\n",
    "**Note that this will only work with the full archive search option**, which is available to my account only via the enterprise options. Full archive search will likely require a different endpoint or access method; please see your developer console for details.\n",
    "\n",
    "Let's make a new rule and pass it dates this time.\n",
    "\n",
    "`gen_rule_payload` takes timestamps of the following forms:\n",
    "\n",
    "\n",
    "- `YYYYmmDDHHMM`\n",
    "- `YYYY-mm-DD` (which will convert to midnight UTC (00:00)\n",
    "- `YYYY-mm-DD HH:MM`\n",
    "- `YYYY-mm-DDTHH:MM`\n",
    "\n",
    "Note - all Tweets are stored in UTC time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = gen_rule_payload(\"from:jack\",\n",
    "                        from_date=\"2017-09-01\", #UTC 2017-09-01 00:00\n",
    "                        to_date=\"2017-10-30\",#UTC 2017-10-30 00:00\n",
    "                        results_per_call=500)\n",
    "print(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = collect_results(rule, max_results=500, result_stream_args=enterprise_search_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(tweet.all_text) for tweet in tweets[0:10]];"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
