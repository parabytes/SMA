{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Initialization\" data-toc-modified-id=\"Initialization-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Initialization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Specify-Global-Variables\" data-toc-modified-id=\"Specify-Global-Variables-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Specify Global Variables</a></span></li><li><span><a href=\"#Functions-and-Classes\" data-toc-modified-id=\"Functions-and-Classes-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Functions and Classes</a></span></li><li><span><a href=\"#System-dependent-Configuration\" data-toc-modified-id=\"System-dependent-Configuration-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>System-dependent Configuration</a></span></li></ul></li><li><span><a href=\"#Collect-Data\" data-toc-modified-id=\"Collect-Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Collect Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setup-the-Data-Collection-Environment\" data-toc-modified-id=\"Setup-the-Data-Collection-Environment-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Setup the Data Collection Environment</a></span></li><li><span><a href=\"#Collect-Instagram-Posts\" data-toc-modified-id=\"Collect-Instagram-Posts-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Collect Instagram Posts</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "<p> This playbooks scrapes Instagram posts for a given hashtag. It can also scrape a particular Instagram post. </p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "\n",
    "<p> The imports, function and class defintions, global variables, and system-dependent configuration are in this section. </p>\n",
    "\n",
    "<p> The system dependent configuration should be carefully reviewed and configured for each system (e.g., Linux vs. Windows, or the path of an external program) since the playbook will most likely fail without proper configuration. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T19:25:07.547341Z",
     "start_time": "2019-08-02T19:24:58.539865Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### This cell imports necessary Python modules and performs initial configuration\n",
    "\n",
    "### Data manipulation libraries\n",
    "# import json\n",
    "import pandas as pd \n",
    "import csv\n",
    "\n",
    "### Visualization and Interaction\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.style.use('ggplot')\n",
    "\n",
    "from IPython.display import set_matplotlib_formats, clear_output\n",
    "set_matplotlib_formats('retina')\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot \n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from ipywidgets import VBox, HBox, Button, HTML\n",
    "\n",
    "### Computation libraries \n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "\n",
    "### Graph analysis\n",
    "# import networkx as nx\n",
    "# import community\n",
    "\n",
    "### System related\n",
    "# import sys\n",
    "# import warnings;\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# import io\n",
    "# from joblib import Parallel, delayed\n",
    "\n",
    "### Datetime libraries\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pytz import timezone\n",
    "\n",
    "### NLP dependencies\n",
    "# import spacy\n",
    "# from spacy.tokenizer import Tokenizer\n",
    "# nlp = spacy.load('en')\n",
    "# tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "# from langdetect import detect\n",
    "\n",
    "### Scraping libraries\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "### Machine learning libraries\n",
    "# from sklearn import datasets\n",
    "# from sklearn import linear_model\n",
    "# from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "### Logging\n",
    "import logging \n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This cell defines global variables and parameters used throughout the playbook\n",
    "\n",
    "# Make this True if you want to watch Selenium scrape pages\n",
    "WATCH_SCRAPING = True\n",
    "\n",
    "RAW_DATA_DIRECTORY = \"../data/raw/\"\n",
    "\n",
    "# Specify the maximum number of thumbnails to scrap\n",
    "MAX_NO_OF_THUMBNAILS_TO_SCRAPE = 5\n",
    "\n",
    "# Specify the maximum number of comments to scrape from a post\n",
    "MAX_NO_OF_COMMENTS_TO_SCRAPE = 10\n",
    "\n",
    "# datetime.now(timezone('US/Eastern')).strftime('%Y-%m-%dT%H:%M:%S%z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T19:27:24.595825Z",
     "start_time": "2019-08-02T19:27:24.560422Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### This cell defines functions and classes used throughout the playbook\n",
    "\n",
    "def scrape_instagram_search(driver, scrape_parameter, type_of_scrape, max_thumbnails=1000, max_scrolls=2000):\n",
    "    \"\"\"Searches for an instagram tag and returns a list of links to thumbnails\"\"\"\n",
    "    url = \"\"\n",
    "    \n",
    "    # For TAG search\n",
    "    if type_of_scrape == \"TAG\":\n",
    "        tag = scrape_parameter.lower().strip()\n",
    "        url = \"https://www.instagram.com/explore/tags/\" + tag\n",
    "\n",
    "    # For LOCATION search\n",
    "    if type_of_scrape == \"LOCATION\":\n",
    "        url = scrape_parameter\n",
    " \n",
    "    driver.get(url)\n",
    "\n",
    "    thumbnail_link_set = []\n",
    "\n",
    "    hit_bottom = False\n",
    "    \n",
    "    thumbnail_wrappers = set()\n",
    "    i = 0\n",
    "    while not hit_bottom and len(thumbnail_link_set) < max_thumbnails and i < max_scrolls:\n",
    "        \n",
    "        i += 1\n",
    "        print(\"Retrieving page number {}\".format(i))\n",
    "        try:\n",
    "            driver.execute_script(\"window.scrollTo(0, \" + str(i * 250) + \");\")\n",
    "            page_html = driver.page_source        \n",
    "            soup = BeautifulSoup(page_html, 'html.parser')\n",
    "            kIKUGs = soup.find_all('div', class_='kIKUG')\n",
    "            for sd in kIKUGs:\n",
    "                thumbnail_wrappers.add(sd)\n",
    "            logging.info(\"Number of thumbnail wrappers = {}\".format(len(thumbnail_wrappers)))\n",
    "            \n",
    "        except StaleElementReferenceException as e:\n",
    "            logging.info('stale element')\n",
    "            logging.info(e)\n",
    "            continue\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info('finished')\n",
    "            logging.info(e)\n",
    "            break\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    if thumbnail_wrappers:\n",
    "        for thumbnail in thumbnail_wrappers:\n",
    "            link_of_the_thumbnail = thumbnail.find('a').get('href')\n",
    "            thumbnail_link_set.append(link_of_the_thumbnail)\n",
    "            \n",
    "    #driver.quit()\n",
    "    logging.info(\"Number of distinct links retrieved = {}\".format(len(thumbnail_link_set)))\n",
    "    return thumbnail_link_set\n",
    "\n",
    "\n",
    "def scrape_instagram_post(driver, post_link, max_comments = 100):\n",
    "    \"\"\"Scrapes an instagram post\"\"\"\n",
    "\n",
    "    # Form the URL\n",
    "    url = \"https://www.instagram.com\" + post_link\n",
    "    \n",
    "    print(url)\n",
    "    driver.get(url)\n",
    "    \n",
    "    tracking_time = datetime.now(timezone('US/Eastern')).strftime('%Y-%m-%dT%H:%M:%S%z')\n",
    "    comment_row = []\n",
    "    post_metadata = {\"post_id\": post_link[3:][:-1],\n",
    "                     \"scraped_datetime\": tracking_time,\n",
    "                     \"post_thumbnail_link\": None,\n",
    "                     \"post_thumbnail_tags\": None,\n",
    "                     \"post_thumbnail_type\": None,\n",
    "                     \"post_number_of_likes\": None,\n",
    "                     \"post_number_of_views\": None,\n",
    "                     \"post_datetime\": None,\n",
    "                     \"user_profile_id\": None,\n",
    "                     \"user_profile_picture\": None,\n",
    "                     \"user_location\": None,\n",
    "                     \"user_verified\": None\n",
    "                    }\n",
    "   \n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            wait = WebDriverWait(driver, 2)\n",
    "            # TODO: Do the load_more comments later.\n",
    "            while False: \n",
    "            # while True:\n",
    "                load_more_comments = wait.until(EC.element_to_be_clickable((By.CLASS_NAME, \"Z4IfV\")))\n",
    "                load_more_comments.click()\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            # logging.info(\"no load more comments button was found\")\n",
    "            # logging.info(e)\n",
    "            \n",
    "        soup = BeautifulSoup(driver.page_source)\n",
    "        \n",
    "        # Check whether it is a video/image\n",
    "        try:\n",
    "            if soup.find(\"div\", class_=\"KL4Bh\"):\n",
    "                \n",
    "                # Check if the img[\"alt\"] exists\n",
    "                try:\n",
    "                    _ = soup.find(\"div\", {\"class\": \"KL4Bh\"}).find(\"img\")[\"alt\"]\n",
    "                    image_alt_exists = True\n",
    "                    logging.info(\"image alt exists\")\n",
    "                except:\n",
    "                    logging.info(\"image alt does not exist\")\n",
    "                    image_alt_exists = False\n",
    "                    \n",
    "                if image_alt_exists:\n",
    "                    post_metadata[\"post_thumbnail_type\"] = \"image\"\n",
    "                    image_container = soup.find(\"div\", {\"class\": \"KL4Bh\"}).find(\"img\")\n",
    "                    image_tags = image_container[\"alt\"]\n",
    "                    image_tags = image_tags.replace(\"Image may contain: \", \"\")\n",
    "                    post_metadata['post_thumbnail_tags'] = image_tags        \n",
    "\n",
    "                    image_src = image_container[\"src\"]\n",
    "                    post_metadata[\"post_thumbnail_link\"] = image_src\n",
    "                else:\n",
    "                    post_metadata[\"post_thumbnail_type\"] = \"video\"\n",
    "                    image_container = soup.find(\"div\", {\"class\": \"KL4Bh\"}).find(\"img\")\n",
    "                    image_src = image_container[\"src\"]\n",
    "                    post_metadata[\"post_thumbnail_link\"] = image_src\n",
    "                    \n",
    "            \"\"\"\n",
    "            if soup.find(\"div\", class_=\"OAXCp\"):\n",
    "                post_metadata[\"post_thumbnail_type\"] = \"video\"\n",
    "                \n",
    "                video_container = soup.find(\"div\", class_=\"OAXCp\").find(\"video\")\n",
    "                video_src = video_container[\"src\"]\n",
    "                post_metadata[\"post_thumbnail_link\"] = video_src\n",
    "            \"\"\"\n",
    "        except:\n",
    "            print(\"Image/video scraping did not work\")\n",
    "            \n",
    "            \n",
    "        # Scrape views and likes\n",
    "        try:\n",
    "            # Check if it is likes\n",
    "            if soup.find(\"div\", class_=\"Nm9Fw\"):\n",
    "                number_of_likes = soup.find(\"div\", class_=\"Nm9Fw\").text.lower().replace(\" likes\", \"\")\n",
    "                post_metadata[\"post_number_of_likes\"] = number_of_likes\n",
    "                \n",
    "            if soup.find(\"div\", class_=\"HbPOm\"):\n",
    "                number_of_views = soup.find(\"div\", class_=\"HbPOm\").text.lower().replace(\" views\", \"\")\n",
    "                post_metadata[\"post_number_of_views\"] = number_of_views     \n",
    "                \n",
    "        except:\n",
    "            print(\"Links/replies scrape did not work\")\n",
    "            \n",
    "        # Scrape time\n",
    "        try:\n",
    "            time_wrapper = soup.find(\"div\", class_=\"NnvRN\")\n",
    "            datetime_of_the_post = time_wrapper.find(\"time\")[\"datetime\"]\n",
    "            post_metadata[\"post_datetime\"] = datetime_of_the_post\n",
    "        except:\n",
    "            print(\"Post datetime scrape did not work\")\n",
    "           \n",
    "        # Scrape profile pic\n",
    "        try:\n",
    "            header_wrapper = soup.find(\"header\", class_=\"UE9AK\")\n",
    "            profile_pic_wrapper = header_wrapper.find(\"div\", class_=\"mrq0Z\")\n",
    "            profile_pic_src = profile_pic_wrapper.find(\"img\")[\"src\"]\n",
    "            post_metadata[\"user_profile_picture\"] = profile_pic_src\n",
    "            \n",
    "            profile_id = header_wrapper.find(\"div\", class_=\"e1e1d\").find(\"a\")[\"title\"]\n",
    "            post_metadata[\"user_profile_id\"] = profile_id\n",
    "        except:\n",
    "            print(\"Profile pic and name scrape did not work\")\n",
    "\n",
    "        # Scrape profile id\n",
    "        try:\n",
    "            profile_id_wrapper = soup.find(\"div\", class_= \"e1e1d\")\n",
    "            profile_id = profile_id_wrapper.find(\"a\")[\"title\"]\n",
    "            post_metadata[\"user_profile_id\"] = profile_id\n",
    "        except:\n",
    "            logging.info(\"Profile id did not work\")\n",
    "            \n",
    "        # Scrape user location    \n",
    "        try:\n",
    "            if soup.find(\"div\", class_=\"M30cS\"):\n",
    "                user_location_wrapper = soup.find(\"div\", class_=\"M30cS\")\n",
    "                user_location = user_location_wrapper.text\n",
    "                if user_location:\n",
    "                    post_metadata[\"user_location\"] = user_location\n",
    "        except:\n",
    "            logging.info(\"User location did not work\")\n",
    "            \n",
    "        # Scrape whether user is verified\n",
    "        try:\n",
    "            profile_id_wrapper = soup.find(\"div\", class_= \"e1e1d\")\n",
    "            if profile_id_wrapper.find(\"span\"):\n",
    "                user_verified = 1\n",
    "            else:\n",
    "                user_verified = 0\n",
    "                \n",
    "            post_metadata[\"user_verified\"] = user_verified\n",
    "        except:\n",
    "            logging.info(\"User verified did not work\")\n",
    "        \n",
    "        try:\n",
    "            user_elements = soup.find(\"div\", class_=\"EtaWk\").find_all(\"div\", class_=\"ZyFrc\")\n",
    "        except:\n",
    "            print(\"Could not find EtaWk and/or ZyFrc\")\n",
    "            \n",
    "        processing_the_first_element = True    \n",
    "        for e in user_elements:\n",
    "            comment_data = {\"user_post_or_comment\": None,\n",
    "                            \"commenter_user_id\": None,\n",
    "                            \"commenter_profile_picture\": None,\n",
    "                            \"commenter_verified\": None,\n",
    "                            \"comment_text\": None,\n",
    "                            \"comment_likes\": None,\n",
    "                            \"comment_replies\": None,\n",
    "                            \"comment_datetime\": None}\n",
    "            \n",
    "            if processing_the_first_element:\n",
    "                comment_data[\"user_post_or_comment\"] = \"user_post\"\n",
    "            else:\n",
    "                comment_data[\"user_post_or_comment\"] = \"comment\"\n",
    "                \n",
    "            element = e.find(\"div\", class_=\"C4VMK\")\n",
    "            try:\n",
    "                comment_text = element.find('span').text\n",
    "                comment_data[\"comment_text\"] = comment_text\n",
    "            except:\n",
    "                print(\"User comment could not be scraped\")\n",
    "                logging.info(\"User comment could not be scraped\")\n",
    "                \n",
    "            try:    \n",
    "                commenter_user_id = element.find(\"a\")[\"title\"]\n",
    "                comment_data[\"commenter_user_id\"] = commenter_user_id\n",
    "            except:\n",
    "                print(\"User id could not be scraped\")\n",
    "                logging.info(\"User id could not be scraped\")\n",
    "\n",
    "            try:\n",
    "                comment_datetime = element.find(\"time\")[\"datetime\"]\n",
    "                comment_data[\"comment_datetime\"] = comment_datetime\n",
    "            except:\n",
    "                logging.info(\"Comment datetime could not be scraped\")\n",
    "            \n",
    "            try:\n",
    "                comment_datetime = element.find(\"time\")[\"datetime\"]\n",
    "                comment_data[\"comment_datetime\"] = comment_datetime\n",
    "            except:\n",
    "                logging.info(\"Comment datetime could not be scraped\")\n",
    "            \n",
    "            try:\n",
    "                comment_buttons = element.find_all(\"button\")\n",
    "                if comment_buttons:\n",
    "                    for cb in comment_buttons:\n",
    "                        text_of_cb = cb.text.lower()\n",
    "                        \n",
    "                        if (\"like\" in text_of_cb) or (\"likes\" in text_of_cb):\n",
    "                            if len(text_of_cb.split()) > 1:\n",
    "                                comment_data[\"comment_likes\"] = text_of_cb.split()[0]\n",
    "\n",
    "                if not processing_the_first_element:\n",
    "                    comment_data[\"comment_replies\"] = 0\n",
    "            except:\n",
    "                print(\"Likes/replies could not be scraped\")\n",
    "                \n",
    "            try:\n",
    "                commenter_profile_picture = e.find(\"div\", class_=\"TKzGu\").find(\"img\")[\"src\"]\n",
    "                comment_data[\"commenter_profile_picture\"] = commenter_profile_picture\n",
    "            except:\n",
    "                logging.info(\"Commenter profile picture could not be scraped\")\n",
    "                              \n",
    "            # rows.append([comment_text.encode(\"utf-8\"), username, numbers[0], numbers[1], numbers[2]])\n",
    "            comment_row.append({**post_metadata, **comment_data})\n",
    "            processing_the_first_element = False    \n",
    "\n",
    "            \n",
    "        logging.info(\"Retrived the post and comments for \" + url)\n",
    "    except Exception as e:\n",
    "        logging.info(\"could not load comments\")\n",
    "        logging.info(e)\n",
    "        \n",
    "    return comment_row  \n",
    "    # return df    \n",
    "        \n",
    "        \n",
    "# TODO: Currently all data is saved into a dataframe and exported to a CSV. \n",
    "# Creating an Instagram class might be useful for further analysis.\n",
    "class InstagramPost:\n",
    "    \"\"\"This class represents an Instagram post.\"\"\"\n",
    "    \n",
    "    def __init__(self, unique_id):\n",
    "        self.unique_id = unique_id\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System-dependent Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T19:25:15.364537Z",
     "start_time": "2019-08-02T19:25:15.361905Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### This cell defines system-dependent configuration such as those different in Linux vs. Windows\n",
    "\n",
    "# Assuming a particular directory structure and a Linux-based system\n",
    "# As of Sep 2, 2019, the chromedriver is version 76.X\n",
    "EXECUTABLE_PATH = \"../WebDriver/chromedriver\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Data Collection Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T19:35:36.040031Z",
     "start_time": "2019-08-02T19:35:34.059901Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:Chrome could not be launched. Check if EXECUTABLE_PATH is configured correcely. If it is, check if the Chromedriver supports the version of the browser.\n"
     ]
    }
   ],
   "source": [
    "### Instagram hashtag or user to be scraped is entered in this step\n",
    "\n",
    "# Create the driver\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "if not WATCH_SCRAPING:\n",
    "    chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--incognito')\n",
    "\n",
    "try:\n",
    "    driver = webdriver.Chrome(options=chrome_options, executable_path=EXECUTABLE_PATH)\n",
    "    logging.info(\"Chrome launched\")\n",
    "except:\n",
    "    logging.critical(\"Chrome could not be launched. Check if EXECUTABLE_PATH is configured correcely. If it is, check if the Chromedriver supports the version of the browser.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Instagram Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T19:59:07.250877Z",
     "start_time": "2019-08-02T19:59:07.206595Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371fbc94562140afbd42ce88e4932c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h1 style='text-align:center'>Instagram Search by Tag</h1>\"), HBox(children=(Dropdoâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd991ed8bf443d19cb16bfbee9de84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### This cell creates the UI for the instagram tag search and retrieves the links for the thumbnails\n",
    "\n",
    "# The dropdown menu that allows the user to search for a hashtag or a user account\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=['Search Tag'],\n",
    "    value='Search Tag',\n",
    "    description='',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Text input box\n",
    "text = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='',\n",
    "    description='',\n",
    "    disabled=False,\n",
    "    style={'description_width': '200px'}, \n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "\n",
    "# The Instagram collection button\n",
    "button = widgets.Button(description=\"Start Collection!\")\n",
    "\n",
    "# Formatting\n",
    "html_text = widgets.HTML(value=\"<h1 style='text-align:center'>Instagram Search by Tag</h1>\")\n",
    "\n",
    "COLUMNS = [\"post_id\", \"scraped_datetime\", \"post_thumbnail_link\", \"post_thumbnail_type\", \"post_thumbnail_tags\", \"post_number_of_likes\", \"post_number_of_views\", \"post_datetime\", \"user_profile_id\", \"user_profile_picture\", \"user_location\", \"user_verified\", \"user_post_or_comment\", \"commenter_user_id\", \"commenter_profile_picture\", \"commenter_verified\", \"comment_text\", \"comment_likes\", \"comment_replies\", \"comment_datetime\"]\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    \"\"\"Triggerred with button click.\"\"\"\n",
    "    global thumbnails, df_user\n",
    "    \n",
    "    with out:\n",
    "        clear_output()\n",
    "        spinner = widgets.HTML(value='<link rel=\"stylesheet\" href=\"https://use.fontawesome.com/releases/v5.8.2/css/all.css\" integrity=\"sha384-oS3vJWv+0UjzBfQzYUhtDYW+Pj2yciDJxpsK1OYPAYjqT085Qq/1cq5FLXAZQ7Ay\" crossorigin=\"anonymous\"><div style=\"margin:auto; width:20%\"><i class=\"fas fa-spinner fa-spin fa-10x\"></i></div>')\n",
    "        display(spinner)\n",
    "        \n",
    "        ### TODO: Scrape the thumbnails\n",
    "        type_of_scrape = \"TAG\"\n",
    "        if dropdown.value == \"Search Tag\":\n",
    "            file_identifier = text.value\n",
    "            type_of_scrape = \"TAG\"\n",
    "        \n",
    "        # TODO: Only works when logged in\n",
    "        if dropdown.value == \"Search by Location\":\n",
    "            file_identifier = text.value.strip(\"/\")[-1]\n",
    "            type_of_scrape = \"LOCATION\"\n",
    "        \n",
    "        df_user = pd.DataFrame(columns=COLUMNS)\n",
    "        thumbnails = scrape_instagram_search(driver, text.value, type_of_scrape)\n",
    "        for thumbnail in thumbnails[:MAX_NO_OF_THUMBNAILS_TO_SCRAPE]:\n",
    "            thumbnail_post_data = scrape_instagram_post(driver, thumbnail)\n",
    "            dummy_df = pd.DataFrame.from_dict(thumbnail_post_data)\n",
    "            df_user = pd.concat([df_user, dummy_df], ignore_index=True, sort=False)\n",
    "        \n",
    "        \n",
    "        df_user.to_csv(RAW_DATA_DIRECTORY + \"INS-\" + type_of_scrape + \"-\" + file_identifier + \"-\" + datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\") + \".csv\", index=False, na_rep='None', columns=COLUMNS)\n",
    "        ### TODO: Scrape the comments\n",
    "        #clear_output()\n",
    "        display(df_user[COLUMNS])\n",
    "        #print(dropdown.value)\n",
    "        #print(text.value)\n",
    "    \n",
    "button.on_click(on_button_clicked)\n",
    "hbox = HBox([dropdown, text, button])\n",
    "vbox = VBox([html_text, hbox])\n",
    "display(vbox)\n",
    "out = widgets.Output()\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-1f415da3c117>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-1f415da3c117>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    driver.quit() and any post-processing is done here\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Add post-processing steps here\n",
    "\"\"\"\n",
    "\n",
    "# Clean up the environment\n",
    "driver.quit() and any post-processing is done here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "215px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
