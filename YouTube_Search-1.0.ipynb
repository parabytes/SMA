{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Functions-and-Classes\" data-toc-modified-id=\"Functions-and-Classes-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Functions and Classes</a></span></li><li><span><a href=\"#System-dependent-Configuration\" data-toc-modified-id=\"System-dependent-Configuration-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>System-dependent Configuration</a></span></li></ul></li><li><span><a href=\"#Collect-Data\" data-toc-modified-id=\"Collect-Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Collect Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Collect-Youtube--Data\" data-toc-modified-id=\"Collect-Youtube--Data-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Collect Youtube  Data</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This playbook has been developed by the Discovery Lab, Applied Intelligence, Accenture Federal Services. @ 2019-2020\n",
    "<p> This playbook harvests metadata and data from the list of videos returned from a YouTube search. </p>\n",
    "\n",
    "<p> <b>INPUT:</b> YouTube search term.</p>\n",
    "\n",
    "<p> <b>OUTPUT</b> is written under data/raw in the format of YOUTUBE_SEARCH_{Scrape_DateTime}_{Search Terms}.csv</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "\n",
    "<p> The imports, function and class defintions, global variables, and system-dependent configuration are in this section. </p>\n",
    "\n",
    "<p> The system dependent configuration should be carefully reviewed and configured for each system (e.g., Linux vs. Windows, or the path of an external program) since the playbook will most likely fail without proper configuration. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"This cell imports necessary Python modules and performs initial configuration\n",
    "\"\"\"\n",
    "\n",
    "### Data libraries\n",
    "import json\n",
    "import pandas as pd \n",
    "import csv\n",
    "\n",
    "### Visualization and Interaction\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.style.use('ggplot')\n",
    "\n",
    "from IPython.display import set_matplotlib_formats, display, clear_output, HTML\n",
    "set_matplotlib_formats('retina')\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot \n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from ipywidgets import VBox, HBox, Button, HTML, Label\n",
    "\n",
    "### Computation libraries \n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "\n",
    "### Graph analysis\n",
    "# import networkx as nx\n",
    "# import community\n",
    "\n",
    "### System related\n",
    "# import sys\n",
    "# import warnings;\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "import io\n",
    "import platform\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# from joblib import Parallel, delayed\n",
    "\n",
    "### Datetime libraries\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pytz import timezone\n",
    "\n",
    "### NLP dependencies\n",
    "# import spacy\n",
    "# from spacy.tokenizer import Tokenizer\n",
    "# nlp = spacy.load('en')\n",
    "# tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "# from langdetect import detect\n",
    "\n",
    "### Scraping libraries\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "### Machine learning libraries\n",
    "# from sklearn import datasets\n",
    "# from sklearn import linear_model\n",
    "# from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "### Logging\n",
    "import logging \n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "#import spacy\n",
    "# nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This cell defines global variables and parameters used throughout the playbook\n",
    "\"\"\"\n",
    "\n",
    "# Set this to True if you want to watch Selenium scrape pages\n",
    "WATCH_SCRAPE = True\n",
    "\n",
    "# Set this to True if you want to use incognito mode\n",
    "USE_INCOGNITO = True\n",
    "\n",
    "# The data is written \n",
    "RAW_DATA_DIRECTORY = Path(\"../data/raw/\")\n",
    "\n",
    "# Change based on how many comments you want to scrape\n",
    "max_num_scrolls = 1\n",
    "\n",
    "# Number of videos to scrape\n",
    "num_vids = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     3,
     85,
     187,
     201
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"This cell defines functions and classes used throughout the playbook\n",
    "\"\"\"\n",
    "\n",
    "def get_vids(search_query, driver, num_results):\n",
    "    youtube_url = 'https://www.youtube.com/results?search_query='\n",
    "    page = youtube_url + search_query\n",
    "    try:\n",
    "        driver.get(page)\n",
    "        logging.info('Retrieving data from ' + page)\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        logging.info('Error retrieving data. Try again.')\n",
    "\n",
    "    # Clear pop up alerts\n",
    "    try:\n",
    "        viewpopup = driver.find_element_by_xpath(\n",
    "            '//ytd-button-renderer[@id=\"dismiss-button\"]/a/paper-button[@id=\"button\"]')\n",
    "        time.sleep(1)\n",
    "        viewpopup.click()\n",
    "        logging.info('Pop up found and cleared')\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        logging.info('No pop up found :)')\n",
    "\n",
    "    # Make soup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    html_txt = soup.prettify()\n",
    "\n",
    "    video_wrappers_max20 = soup.find_all('div', attrs={'id': 'dismissable', 'class': 'style-scope ytd-video-renderer'})\n",
    "    video_wrappers = video_wrappers_max20[:num_results]\n",
    "    video_row = []\n",
    "\n",
    "    for video in video_wrappers:\n",
    "        # Find original comment\n",
    "        original = video.find_all('div', attrs={'class': 'text-wrapper style-scope ytd-video-renderer'})\n",
    "        video_data = {'title': None,\n",
    "                      'owner': None,\n",
    "                      'owner_channel': None,\n",
    "                      'url': None\n",
    "                      }\n",
    "\n",
    "        # Get author name\n",
    "        try:\n",
    "            for b in original:\n",
    "                video_data['owner'] = b.find(name=\"a\", attrs={\n",
    "                    \"class\": \"yt-simple-endpoint style-scope yt-formatted-string\"}).text.strip()\n",
    "        except:\n",
    "            logging.info('No owner found')\n",
    "\n",
    "        # Get author channel link\n",
    "        try:\n",
    "            for d in original:\n",
    "                link = d.find(name=\"a\",\n",
    "                              attrs={\"class\": \"yt-simple-endpoint style-scope yt-formatted-string\", \"href\": True})\n",
    "                channel = \"https://youtube.com\" + link['href']\n",
    "                video_data['owner_channel'] = channel\n",
    "        except:\n",
    "            logging.info('No owner channel link found')\n",
    "\n",
    "        # Get title\n",
    "        try:\n",
    "            for c in original:\n",
    "                video_data['title'] = c.find(name=\"yt-formatted-string\",\n",
    "                                             attrs={\"class\": \"style-scope ytd-video-renderer\"}).text.strip()\n",
    "        except:\n",
    "            logging.info('No title found')\n",
    "\n",
    "        # Get vid url\n",
    "        try:\n",
    "            for a in original:\n",
    "                title_link = a.find(name=\"a\", attrs={\"id\": \"video-title\", \"href\": True})\n",
    "                url = \"https://youtube.com\" + title_link['href']\n",
    "                video_data['url'] = url\n",
    "        except:\n",
    "            logging.info('No url found')\n",
    "\n",
    "        video_row.append(video_data)\n",
    "\n",
    "    videodf = pd.DataFrame(video_row)\n",
    "    videodf['vid_index'] = videodf.index + 1\n",
    "    # print(videodf)\n",
    "    vid_urls = videodf['url'].tolist()\n",
    "    return videodf, vid_urls\n",
    "\n",
    "\n",
    "def get_channel_vids(channel_list, driver, num_results):\n",
    "    channel_vid_urls = []\n",
    "    channel_df = []\n",
    "    for page in channel_list:\n",
    "        try:\n",
    "            driver.get(page + '/videos')\n",
    "            logging.info('Retrieving data from ' + page + '/videos')\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            logging.info('Error retrieving data. Try again.')\n",
    "\n",
    "        if num_results > 30:\n",
    "            logging.info('scrolling down for more videos')\n",
    "            driver.execute_script('window.scrollBy(0,1500)')\n",
    "            time.sleep(2)\n",
    "            if num_results > 60:\n",
    "                logging.info('scrolling down for more videos')\n",
    "                driver.execute_script('window.scrollBy(0,1500)')\n",
    "                time.sleep(2)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        video_wrappers_all = soup.find_all('div',\n",
    "                                           attrs={'id': 'details', 'class': 'style-scope ytd-grid-video-renderer'})\n",
    "        video_wrappers = video_wrappers_all[:num_results]\n",
    "        video_row = []\n",
    "\n",
    "        if video_wrappers:\n",
    "            for video in video_wrappers:\n",
    "                video_data = {'title': None,\n",
    "                              'url': None\n",
    "                              }\n",
    "                # Get title\n",
    "                try:\n",
    "                    video_data['title'] = video.find(name=\"a\",\n",
    "                                                     attrs={\"id\": \"video-title\",\n",
    "                                                            \"class\": \"yt-simple-endpoint style-scope ytd-grid-video-renderer\"}).text.strip()\n",
    "                except:\n",
    "                    logging.info('No title found')\n",
    "\n",
    "                # Get vid url\n",
    "                try:\n",
    "                    title_link = video.find(name=\"a\", attrs={\"id\": \"video-title\", \"href\": True})\n",
    "                    url = \"https://youtube.com\" + title_link['href']\n",
    "                    video_data['url'] = url\n",
    "                except:\n",
    "                    logging.info('No url found')\n",
    "\n",
    "                video_row.append(video_data)\n",
    "            videodf = pd.DataFrame(video_row)\n",
    "            vid_urls = videodf['url'].tolist()\n",
    "            channel_vid_urls.extend(vid_urls)\n",
    "\n",
    "        try:\n",
    "            driver.get(page + '/about')\n",
    "            logging.info('Retrieving data from ' + page + '/about')\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            logging.info('Error retrieving data. Try again.')\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        # html_txt = soup.prettify()\n",
    "\n",
    "        channel_name = soup.find(name=\"yt-formatted-string\",\n",
    "                                 attrs={\"id\": \"text\", \"class\": \"style-scope ytd-channel-name\"}).text.strip()\n",
    "        description = soup.find(name=\"yt-formatted-string\", attrs={\"id\": \"description\",\n",
    "                                                                   \"class\": \"style-scope ytd-channel-about-metadata-renderer\"}).text.strip()\n",
    "\n",
    "        stats = \\\n",
    "            soup.find_all('div',\n",
    "                          attrs={\"id\": \"right-column\", 'class': 'style-scope ytd-channel-about-metadata-renderer'})[\n",
    "                0].text.strip()\n",
    "\n",
    "        location = soup.find_all('tr', attrs={'class': 'style-scope ytd-channel-about-metadata-renderer'})[\n",
    "            -1].text.strip()\n",
    "        links = soup.find_all('a', attrs={\"class\": \"yt-simple-endpoint style-scope ytd-channel-about-metadata-renderer\",\n",
    "                                          \"href\": True})\n",
    "        links_url = []\n",
    "        for i in links:\n",
    "            links_url.append(i['href'])\n",
    "\n",
    "        channel_dict = {'channel_name': None,\n",
    "                        'description': None,\n",
    "                        'stats': None,\n",
    "                        'location': None\n",
    "                        }\n",
    "\n",
    "        channel_dict['channel_name'] = channel_name\n",
    "        channel_dict['description'] = description\n",
    "        channel_dict['stats'] = stats\n",
    "        channel_dict['location'] = location\n",
    "        channeldf = pd.DataFrame([channel_dict])\n",
    "        for i in range(len(links_url)):\n",
    "            channeldf['link_url_' + str(i)] = links_url[i]\n",
    "        channel_df.append(channeldf)\n",
    "    channel_info_df = pd.concat(channel_df, ignore_index=True)\n",
    "\n",
    "    filename = \"YOUTUBE_\" + datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\") + \"_CHANNEL\" + \".xlsx\"\n",
    "    channel_info_df.to_excel(str(raw_data_dir / filename), index=False, na_rep='None', encoding='UTF-16')\n",
    "\n",
    "    return channel_info_df, channel_vid_urls\n",
    "\n",
    "\n",
    "def to_excel_file(df, raw_data_directory, name_str):\n",
    "    interval = 200000\n",
    "    df_length = len(df.index)\n",
    "    if df_length > interval:\n",
    "        print(np.ceil(df_length / interval))\n",
    "        array_of_df = np.array_split(df, np.ceil(df_length / interval))\n",
    "        for idx, df_i in enumerate(array_of_df):\n",
    "            file_name = \"YOUTUBE_\" + datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\") + name_str + \".xlsx\"\n",
    "            df_i.to_excel(str(raw_data_directory / file_name), index=False, na_rep='None', encoding='UTF-16')\n",
    "    else:\n",
    "        file_name = \"YOUTUBE_\" + datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\") + name_str + \".xlsx\"\n",
    "        df.to_excel(str(raw_data_directory / file_name), index=False, na_rep='None', encoding='UTF-16')\n",
    "\n",
    "\n",
    "def yt_scrape(url_list, url_shortened, raw_data_directory, driver, max_scrolls):\n",
    "    scrape_time = datetime.now()\n",
    "    vid_row = []\n",
    "    comment_row = []\n",
    "    for page in url_list:\n",
    "        # metadata #############################################################\n",
    "        # <editor-fold desc=\"Metadata\">\n",
    "        try:\n",
    "            if url_shortened:\n",
    "                page = 'https://www.youtube.com' + page\n",
    "            driver.get(page)\n",
    "            logging.info('Retrieving data from ' + page)\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            logging.info('Error retrieving data. Try again.')\n",
    "\n",
    "        # Clear pop up alerts\n",
    "        try:\n",
    "            viewpopup = driver.find_element_by_xpath(\n",
    "                '//ytd-button-renderer[@id=\"dismiss-button\"]/a/paper-button[@id=\"button\"]')\n",
    "            time.sleep(1)\n",
    "            viewpopup.click()\n",
    "            logging.info('Pop up found and cleared')\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            logging.info('No pop up found :)')\n",
    "\n",
    "        # Open transcript\n",
    "        open_transcript = False\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            menu = driver.find_element_by_xpath(\n",
    "                '//div[@id=\"menu-container\"]/div/ytd-menu-renderer/yt-icon-button/button[@id=\"button\"]')\n",
    "            time.sleep(1)\n",
    "            menu.click()\n",
    "            logging.info('Opened menu')\n",
    "            try:\n",
    "                opentranscript = driver.find_element_by_xpath(\n",
    "                    '//ytd-menu-popup-renderer/paper-listbox/ytd-menu-service-item-renderer/paper-item[@class=\"style-scope ytd-menu-service-item-renderer\"]')\n",
    "                opentranscript.click()\n",
    "                logging.info('Opened video transcript')\n",
    "                open_transcript = True\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                logging.info('No transcript found')\n",
    "                time.sleep(1)\n",
    "        except:\n",
    "            logging.info('Cannot open menu')\n",
    "            time.sleep(1)\n",
    "\n",
    "        # Open show more description\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            description = driver.find_element_by_xpath('//ytd-expander/paper-button[@id=\"more\"]')\n",
    "            description.click()\n",
    "            logging.info('Showing more description')\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            logging.info('Cannot show more description')\n",
    "\n",
    "        # # Scroll down past comment header\n",
    "        # driver.execute_script('window.scrollTo(0,500)')\n",
    "        # time.sleep(1)\n",
    "        #\n",
    "        # # Scroll to comment header\n",
    "        # try:\n",
    "        #     commentheader = driver.find_element_by_xpath(\n",
    "        #         '//paper-button[@class=\"dropdown-trigger style-scope yt-dropdown-menu\"]')\n",
    "        # except:\n",
    "        #     try:\n",
    "        #         logging.info('scrolling more to find comment order dropdown')\n",
    "        #         driver.execute_script('window.scrollBy(0,500)')\n",
    "        #         time.sleep(1)\n",
    "        #         commentheader = driver.find_element_by_xpath(\n",
    "        #             '//paper-button[@class=\"dropdown-trigger style-scope yt-dropdown-menu\"]')\n",
    "        #     except:\n",
    "        #         logging.info('scrolling even more to find comment order dropdown')\n",
    "        #         driver.execute_script('window.scrollBy(0,500)')\n",
    "        #         time.sleep(1)\n",
    "        #         commentheader = driver.find_element_by_xpath(\n",
    "        #             '//paper-button[@class=\"dropdown-trigger style-scope yt-dropdown-menu\"]')\n",
    "        #\n",
    "        #\n",
    "        # action = ActionChains(driver)\n",
    "        # action.move_to_element(commentheader).perform()\n",
    "        # time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            title = driver.find_element_by_xpath('//*[@id=\"container\"]/h1/yt-formatted-string')\n",
    "            title.click()\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            logging.info(\"no title found?\")\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        # html_txt = soup.prettify()\n",
    "\n",
    "        vid_metadata = {'url': \"Original Comment\",\n",
    "                        'title': None,\n",
    "                        'scrape_time': None,\n",
    "                        'datetime_yt': None,\n",
    "                        'datetime_adj': None,\n",
    "                        'duration': None,\n",
    "                        'views': None,\n",
    "                        'votes': None,\n",
    "                        'num_comments': None,\n",
    "                        'owner': None,\n",
    "                        'owner_channel': None,\n",
    "                        'owner_subscribers': None,\n",
    "                        'description': None,\n",
    "                        'type': None,\n",
    "                        'transcript': None,\n",
    "                        }\n",
    "\n",
    "        # For number formatting\n",
    "        multipliers = {'K': 1000, 'M': 1000000, 'B': 1000000000}\n",
    "\n",
    "        # Get URL\n",
    "        vid_metadata['url'] = page\n",
    "\n",
    "        # Get title\n",
    "        try:\n",
    "            title = soup.find(name=\"h1\",\n",
    "                              attrs={\"class\": \"title style-scope ytd-video-primary-info-renderer\"}).text.strip()\n",
    "            vid_metadata['title'] = title\n",
    "        except:\n",
    "            logging.info('No title found')\n",
    "\n",
    "        # Get date & set video type\n",
    "        try:\n",
    "            vid_metadata['type'] = 'Video'\n",
    "            date = soup.find(name=\"div\", attrs={\"id\": \"date\"}).text.strip()\n",
    "            date = re.sub(u\"\\u2022\", '', date)\n",
    "            if \"tream\" in date:\n",
    "                vid_metadata['type'] = 'Livestream'\n",
    "            date = date.strip(\"Streamed live on \").strip(\"Started streaming on \").strip(\"Started streaming \").strip(\n",
    "                \"Streamed live \").strip(\"Premiered \")\n",
    "            adj = datetime.strptime(date, '%b %d, %Y')\n",
    "            adj = adj.strftime(\"%m/%d/%Y\")\n",
    "            vid_metadata['datetime_yt'] = date\n",
    "            vid_metadata['datetime_adj'] = adj\n",
    "        except:\n",
    "            logging.info('No date found')\n",
    "\n",
    "        # Get video duration\n",
    "        try:\n",
    "            duration = soup.find(name=\"span\", attrs={\"class\": \"ytp-time-duration\"}).text.strip()\n",
    "            duration_formatted = datetime.strptime(duration, '%M:%S').time()\n",
    "            vid_metadata['duration'] = duration_formatted\n",
    "        except:\n",
    "            logging.info('duration is ' + str(duration) + \", trying H:M:S\")\n",
    "            try:\n",
    "                duration_formatted = datetime.strptime(duration, '%H:%M:%S').time()\n",
    "                vid_metadata['duration'] = duration_formatted\n",
    "            except:\n",
    "                logging.info('duration is ' + str(duration) + \", trying D:H:M:S\")\n",
    "                try:\n",
    "                    duration_formatted = datetime.strptime(duration, '%D:%H:%M:%S').time()\n",
    "                    vid_metadata['duration'] = duration_formatted\n",
    "                except:\n",
    "                    logging.info('duration is ' + str(duration) + \", unknown format\")\n",
    "\n",
    "        # Get views\n",
    "        try:\n",
    "            views = soup.find(name=\"span\",\n",
    "                              attrs={\"class\": \"view-count style-scope yt-view-count-renderer\"}).text.strip()\n",
    "            views = views.strip(' views').strip(' watching now')\n",
    "            views = int(views.replace(\",\", \"\"))\n",
    "            vid_metadata['views'] = views\n",
    "        except:\n",
    "            logging.info('No views found')\n",
    "\n",
    "        # Get votes\n",
    "        try:\n",
    "            votes = []\n",
    "            for a in soup.find_all(name=\"ytd-toggle-button-renderer\"):\n",
    "                vote = a.find(name=\"yt-formatted-string\",\n",
    "                              attrs={\"class\": \"style-scope ytd-toggle-button-renderer style-text\", \"aria-label\": True})\n",
    "                values = vote[\"aria-label\"].strip(\" dislikes\")\n",
    "                values = int(values.replace(\",\", \"\"))\n",
    "                votes.append(values)\n",
    "                vid_metadata['votes'] = votes\n",
    "        except:\n",
    "            logging.info('No votes found')\n",
    "\n",
    "        # Get owner/channel\n",
    "        try:\n",
    "            owner = soup.find(name=\"ytd-channel-name\", attrs={\"id\": \"channel-name\"})\n",
    "            channel = soup.find(name=\"a\",\n",
    "                                attrs={\"class\": \"yt-simple-endpoint style-scope yt-formatted-string\", \"href\": True})\n",
    "            vid_metadata['owner_channel'] = \"https://youtube.com\" + channel[\"href\"]\n",
    "            vid_metadata['owner'] = owner.text.replace(\"\\n\", \"\")\n",
    "        except:\n",
    "            logging.info('No owner found')\n",
    "\n",
    "        # Get owner subscriber\n",
    "        try:\n",
    "            subscribers = soup.find(name=\"yt-formatted-string\", attrs={\"id\": \"owner-sub-count\"}).text.strip()\n",
    "            subscribers = subscribers.strip(\" subscribers\")\n",
    "            if subscribers[-1].isdigit():\n",
    "                formatted = int(subscribers)\n",
    "            else:\n",
    "                mult = multipliers[subscribers[-1]]\n",
    "                formatted = int(float(subscribers[:-1]) * mult)\n",
    "            vid_metadata['owner_subscribers'] = formatted\n",
    "        except:\n",
    "            logging.info('No owner subscriber count found')\n",
    "\n",
    "        # Get description\n",
    "        try:\n",
    "            description = soup.find(name=\"yt-formatted-string\", attrs={\n",
    "                \"class\": \"content style-scope ytd-video-secondary-info-renderer\"}).text.strip()\n",
    "            vid_metadata['description'] = description\n",
    "        except:\n",
    "            logging.info('No description found')\n",
    "\n",
    "        # Get transcript\n",
    "        try:\n",
    "            if open_transcript:\n",
    "                transcript = soup.find_all(name=\"ytd-transcript-body-renderer\")[0].text.replace(\"\\n\", \"\")\n",
    "                vid_metadata['transcript'] = transcript\n",
    "            else:\n",
    "                logging.info('No transcript found')\n",
    "        except:\n",
    "            logging.info('No transcript found')\n",
    "\n",
    "        driver.execute_script('window.scrollBy(0,1100)')\n",
    "        time.sleep(3)\n",
    "        # Get number of comments\n",
    "        try:\n",
    "            # num = driver.find_element_by_xpath(\n",
    "            #    \"/html/body/ytd-app/div/ytd-page-manager/ytd-watch-flexy/div[4]/div[1]/div/ytd-comments/ytd-item-section-renderer/div[1]/ytd-comments-header-renderer/div[1]/h2/yt-formatted-string\")\n",
    "            # num = driver.find_element_by_xpath('//*[@id=\"count\"]/yt-formatted-string')\n",
    "            # driver.execute_script(\"arguments[0].scrollIntoView(false);\", num)\n",
    "            # time.sleep(1)\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            num = soup.find(name=\"h2\",\n",
    "                            attrs={\"id\": \"count\", \"class\": \"style-scope ytd-comments-header-renderer\"}).text.strip()\n",
    "            num = num.strip(' Comments')\n",
    "            num = int(num.replace(\",\", \"\"))\n",
    "            vid_metadata['num_comments'] = num\n",
    "        except:\n",
    "            try:\n",
    "                description_button = driver.find_element_by_xpath('//*[@id=\"less\"]/yt-formatted-string')\n",
    "                description_button.click()\n",
    "                driver.execute_script('window.scrollBy(0,-700)')\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                logging.info(\"no description expansion found\")\n",
    "            logging.info('No comments found, scrolling up just in case')\n",
    "            driver.execute_script('window.scrollBy(0,-700)')\n",
    "            time.sleep(3)\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            try:\n",
    "                num = driver.find_element_by_xpath(\n",
    "                    '/html/body/ytd-app/div/ytd-page-manager/ytd-watch-flexy/div[4]/div[1]/div/ytd-comments/ytd-item-section-renderer/div[1]/ytd-comments-header-renderer/div[1]/h2/yt-formatted-string')\n",
    "                print(\"selenium execute script\")\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView(true);\", num)\n",
    "                time.sleep(1)\n",
    "                # driver.execute_script(\"arguments[0].scrollIntoView(false);\", num)\n",
    "                num = soup.find(name=\"h2\",\n",
    "                                attrs={\"id\": \"count\", \"class\": \"style-scope ytd-comments-header-renderer\"}).text.strip()\n",
    "                # num = soup.find_all(name=\"yt-formatted-string\", attrs={\"class\": \"count-text style-scope ytd-comments-header-renderer\"}) #.text.strip()\n",
    "                num = num.strip(' Comments')\n",
    "                num = int(num.replace(\",\", \"\"))\n",
    "                vid_metadata['num_comments'] = num\n",
    "            except:\n",
    "                try:\n",
    "                    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                    num = soup.find(name=\"h2\",\n",
    "                                    attrs={\"id\": \"count\",\n",
    "                                           \"class\": \"style-scope ytd-comments-header-renderer\"}).text.strip()\n",
    "                    # num = soup.find_all(name=\"yt-formatted-string\", attrs={\"class\": \"count-text style-scope ytd-comments-header-renderer\"}) #.text.strip()\n",
    "                    # num = driver.find_element_by_xpath('//*[@id=\"count\"]/yt-formatted-string')\n",
    "                    # driver.execute_script(\"arguments[0].scrollIntoView(false);\", num)\n",
    "                    time.sleep(1)\n",
    "                    num = num.strip(' Comments')\n",
    "                    num = int(num.replace(\",\", \"\"))\n",
    "                    vid_metadata['num_comments'] = num\n",
    "                except:\n",
    "                    logging.info('No comments found, scrolling down more just in case FOR REAL')\n",
    "                    driver.execute_script('window.scrollBy(0,1100)')\n",
    "                    time.sleep(5)\n",
    "                    try:\n",
    "                        logging.info(\"last attempt to scrape comment count\")\n",
    "                        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                        num = soup.find(name=\"h2\",\n",
    "                                        attrs={\"id\": \"count\",\n",
    "                                               \"class\": \"style-scope ytd-comments-header-renderer\"}).text.strip()\n",
    "                        # num = soup.find_all(name=\"yt-formatted-string\", attrs={\"class\": \"count-text style-scope ytd-comments-header-renderer\"}) #.text.strip()\n",
    "                        # num = driver.find_element_by_xpath('//*[@id=\"count\"]/yt-formatted-string')\n",
    "                        # driver.execute_script(\"arguments[0].scrollIntoView(false);\", num)\n",
    "                        time.sleep(1)\n",
    "                        num = num.strip(' Comments')\n",
    "                        num = int(num.replace(\",\", \"\"))\n",
    "                        vid_metadata['num_comments'] = num\n",
    "                    except:\n",
    "                        logging.info('No comments found')\n",
    "\n",
    "        vid_metadata['scrape_time'] = scrape_time\n",
    "        vid_row.append(vid_metadata)\n",
    "        # </editor-fold>\n",
    "\n",
    "        # comments #############################################################\n",
    "        # <editor-fold desc=\"Comments\">\n",
    "        print('Retrieving comments from ' + str(page))\n",
    "\n",
    "        # Scroll to past comment header\n",
    "        driver.execute_script('window.scrollTo(0,500)')\n",
    "        time.sleep(1)\n",
    "\n",
    "        # <editor-fold desc=\"Sort by newest first\">\n",
    "        # # Scroll to \"Sort By\" drop down\n",
    "        # sortcomment = driver.find_element_by_xpath(\n",
    "        #     '//paper-button[@class=\"dropdown-trigger style-scope yt-dropdown-menu\"]')\n",
    "        # action = ActionChains(driver)\n",
    "        # action.move_to_element(sortcomment).perform()\n",
    "        #\n",
    "        # # Sort comments by newest first\n",
    "        # try:\n",
    "        #     sortcomment.click()\n",
    "        #     time.sleep(1)\n",
    "        #     newestfirst = driver.find_element_by_xpath(\n",
    "        #         '//a[@class=\"yt-simple-endpoint style-scope yt-dropdown-menu\"]/paper-item[@class=\"style-scope yt-dropdown-menu\"]')\n",
    "        #     newestfirst.click()\n",
    "        #     logging.info(\n",
    "        #         'Sorted comments by newest first')  # Selected sort view has CSS selector \"yt-simple-endpoint style-scope yt-dropdown menu iron-selected\" - Default is by Most Relevant\n",
    "        #     time.sleep(1)\n",
    "        # except:\n",
    "        #     driver.execute_script('window.scrollTo(0,{0})').format(scroll_down * 100)\n",
    "        #     logging.info('Cannot sort comments')\n",
    "        #     time.sleep(1)\n",
    "        # </editor-fold>\n",
    "\n",
    "        # Scroll down page\n",
    "        scroll_down = 1\n",
    "        while scroll_down <= max_scrolls:  # Max scrolls defined in System-Dependent Configurations\n",
    "            driver.execute_script(\"window.scrollTo(0,{0})\".format(scroll_down * 100000))\n",
    "            scroll_down += 1\n",
    "            time.sleep(5)\n",
    "\n",
    "        replies_div = driver.find_elements_by_xpath(\n",
    "            '//ytd-button-renderer[@id=\"more-replies\"]/a/paper-button[@id=\"button\"]')\n",
    "        morereplies_div = driver.find_elements_by_xpath(\n",
    "            '//div[@id=\"expander-contents\"]/div/yt-next-continuation/paper-button[@role=\"button\"]')\n",
    "\n",
    "        for reply in replies_div:\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView(false);\", reply)\n",
    "                reply.click()\n",
    "                logging.info(\"Replies found and clicked\")\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                logging.info(\"Replies not found, trying again\")\n",
    "                try:\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView(true);\", reply)\n",
    "                    reply.click()\n",
    "                    logging.info(\"Replies found and clicked\")\n",
    "                    time.sleep(2)\n",
    "                except:\n",
    "                    logging.info(\"Replies not found\")\n",
    "\n",
    "        for more in morereplies_div:\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView(false);\", more)\n",
    "                more.click()\n",
    "                logging.info(\"More replies found and clicked\")\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                logging.info(\"More replies not found\")\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        comment_wrappers = soup.find_all('ytd-comment-thread-renderer',\n",
    "                                         attrs={'class': 'style-scope ytd-item-section-renderer'})\n",
    "        # comment_wrappers2 = soup.select('#contents > ytd-comment-thread-renderer')\n",
    "        for comment in comment_wrappers:\n",
    "            # Find original comment\n",
    "            original = comment.find_all(name='ytd-comment-renderer', attrs={'id': 'comment'})\n",
    "            comment_data = {'type': \"Original Comment\",\n",
    "                            'scrape_time': None,\n",
    "                            'datetime_yt': None,\n",
    "                            'datetime_adj': None,\n",
    "                            'author': None,\n",
    "                            'author_channel': None,\n",
    "                            'text': None,\n",
    "                            'comment_votes': None,\n",
    "                            'url': page\n",
    "                            }\n",
    "            # Get time of post\n",
    "            try:\n",
    "                for a in original:\n",
    "                    dt = a.find(name=\"a\",\n",
    "                                attrs={\"class\": \"yt-simple-endpoint style-scope yt-formatted-string\"}).text.strip()\n",
    "                    comment_data['datetime_yt'] = dt\n",
    "                    comment_data['datetime_adj'] = dateparser.parse(dt.rstrip(\"(edited)\")).strftime(\"%m/%d/%Y\")\n",
    "            except:\n",
    "                logging.info('No datetime found')\n",
    "\n",
    "            # Get author name\n",
    "            try:\n",
    "                for b in original:\n",
    "                    comment_data['author'] = b.find(name=\"a\", attrs={\"id\": \"author-text\"}).text.strip()\n",
    "            except:\n",
    "                logging.info('No author found')\n",
    "\n",
    "            # Get author channel link\n",
    "            try:\n",
    "                for d in original:\n",
    "                    link = d.find(name=\"a\", attrs={\"id\": \"author-text\", \"href\": True})\n",
    "                    channel = \"https://youtube.com\" + link['href']\n",
    "                    comment_data['author_channel'] = channel\n",
    "            except:\n",
    "                logging.info('No author channel link found')\n",
    "\n",
    "            # Get comment\n",
    "            try:\n",
    "                for c in original:\n",
    "                    comment_data['text'] = c.find(name=\"yt-formatted-string\",\n",
    "                                                  attrs={\"id\": \"content-text\"}).text.strip()\n",
    "            except:\n",
    "                logging.info('No comment found')\n",
    "\n",
    "            # Get votes\n",
    "            try:\n",
    "                for e in original:\n",
    "                    # votes = e.find_all(\"span\", attrs={\"class\": \"style-scope ytd-comment-action-buttons-renderer\", \"id\": \"vote-count-middle\"})\n",
    "                    votes = e.find(name=\"span\", attrs={\"id\": \"vote-count-middle\"}).text.strip()\n",
    "                    comment_data['comment_votes'] = votes\n",
    "            except:\n",
    "                logging.info('No votes found')\n",
    "\n",
    "            comment_data['scrape_time'] = scrape_time\n",
    "            comment_df = pd.DataFrame([comment_data])\n",
    "\n",
    "            # Find replies\n",
    "            # reply = driver.find_element_by_xpath('//*[@id=\"loaded-replies\"]/ytd-comment-renderer[1]')\n",
    "            # reply = comment.find_all()\n",
    "            reply = comment.find_all('ytd-comment-renderer',\n",
    "                                     attrs={'class': 'style-scope ytd-comment-replies-renderer'})\n",
    "            for post in reply:\n",
    "                reply_data = {'type': \"Reply\",\n",
    "                              'scrape_time': None,\n",
    "                              'datetime_yt': None,\n",
    "                              'datetime_adj': None,\n",
    "                              'author': None,\n",
    "                              'author_channel': None,\n",
    "                              'text': None,\n",
    "                              'comment_votes': None,\n",
    "                              'url': page\n",
    "                              }\n",
    "\n",
    "                # Get time of post\n",
    "                try:\n",
    "                    dt = post.find(name=\"a\", attrs={\n",
    "                        \"class\": \"yt-simple-endpoint style-scope yt-formatted-string\"}).text.strip()\n",
    "                    reply_data['datetime_yt'] = dt\n",
    "                    reply_data['datetime_adj'] = dateparser.parse(dt.rstrip(\"(edited)\")).strftime(\"%m/%d/%Y\")\n",
    "                except:\n",
    "                    logging.info('No datetime found')\n",
    "\n",
    "                # Get author\n",
    "                try:\n",
    "                    reply_data['author'] = post.find(name=\"a\", attrs={\"id\": \"author-text\"}).text.strip()\n",
    "                except:\n",
    "                    logging.info('No author found')\n",
    "\n",
    "                # Get author channel link\n",
    "                try:\n",
    "                    link = post.find(name=\"a\", attrs={\"id\": \"author-text\", \"href\": True})\n",
    "                    channel = \"https://youtube.com\" + link['href']\n",
    "                    reply_data['author_channel'] = channel\n",
    "                except:\n",
    "                    logging.info('No author channel link found')\n",
    "\n",
    "                # Get comment\n",
    "                try:\n",
    "                    reply_data['text'] = post.find(name=\"yt-formatted-string\",\n",
    "                                                   attrs={\"id\": \"content-text\"}).text.strip()\n",
    "                except:\n",
    "                    logging.info('No comment found')\n",
    "\n",
    "                # Get votes\n",
    "                try:\n",
    "                    votes = post.find(name=\"span\", attrs={\"id\": \"vote-count-middle\"}).text.strip()\n",
    "                    reply_data['comment_votes'] = int(votes)\n",
    "                except:\n",
    "                    logging.info('No votes found')\n",
    "\n",
    "                reply_df = pd.DataFrame([reply_data])\n",
    "                reply_df['scrape_time'] = scrape_time\n",
    "                \n",
    "\n",
    "                comment_df = pd.concat([comment_df, reply_df], ignore_index=True)\n",
    "\n",
    "            comment_row.append(comment_df)\n",
    "        # </editor-fold>\n",
    "\n",
    "    metadf = pd.DataFrame.from_dict(vid_row)\n",
    "    print(metadf.head())\n",
    "    df_type = '_METADATA'\n",
    "    # metadf.to_excel(str(raw_data_directory / file_name), index=False, na_rep='None', encoding='UTF-16')\n",
    "    to_excel_file(metadf, raw_data_directory, df_type)\n",
    "\n",
    "    commentdf = pd.concat(comment_row, ignore_index=True)\n",
    "    commentdf['comment_index'] = commentdf.index + 1\n",
    "    print(commentdf.head())\n",
    "    df_type = '_VIDEO'\n",
    "    # commentdf.to_excel(str(raw_data_directory / file_name), index=False, na_rep='None', encoding='UTF-16')\n",
    "    to_excel_file(commentdf, raw_data_directory, df_type)\n",
    "\n",
    "\n",
    "def divide_list(l, n):\n",
    "    # looping till length l\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "\n",
    "def yt_scrape_lots_of_videos(url_list, url_shortened):\n",
    "    # How many elements each list should have\n",
    "    num_urls = 2000\n",
    "    total_urls = len(url_list)\n",
    "    x = list(divide_list(url_list, num_urls))\n",
    "    for idx, url_sublist in enumerate(x):\n",
    "        yt_scrape(url_sublist, url_shortened, raw_data_dir, web_driver, max_num_scrolls)\n",
    "        print('num of urls is', str(total_urls), ' and finished round ', idx)\n",
    "\n",
    "\n",
    "def strip_tag(list_obj):\n",
    "    ls = [x.strip('/watch?v=') for x in list_obj]\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System-dependent Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This cell defines system-dependent configuration such as those different in Linux vs. Windows\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# System dependent configuration\n",
    "os.getcwd()\n",
    "PLATFORM_SYSTEM = platform.system()\n",
    "\n",
    "if PLATFORM_SYSTEM == \"Darwin\":\n",
    "    # EXECUTABLE_PATH = Path(\"../dependencies/chromedriver\")\n",
    "    EXECUTABLE_PATH = Path(\n",
    "        \"../dependencies/chromedriver\")\n",
    "elif PLATFORM_SYSTEM == \"Windows\":\n",
    "    EXECUTABLE_PATH = Path(\"~/../dependencies\")\n",
    "    file = EXECUTABLE_PATH / \"chromedriver.exe\"\n",
    "else:\n",
    "    logging.critical(\"System not supported...\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Youtube  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     23,
     26
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the YouTube search term: Trump\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:Chrome could not be launched. Check executable path and if Chromedriver supports the version of the browser.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'web_driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-248c9ec7f09c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# or get it from the search results for a search query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0msearchresultsdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweb_driver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_vids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# or get it from the uploaded videos from a list of channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'web_driver' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"This cell retrieves page posts and comments, for a given page.\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "##########################################\n",
    "# step 0: set up\n",
    "    # Global variables and parametrs used throughout the notebook\n",
    "    # Make true if you want to watch scrape\n",
    "\n",
    "    search_term = input(\"Enter the YouTube search term: \")\n",
    "    \n",
    "    # Update when running\n",
    "    raw_data_dir = Path(\n",
    "        \"../data/raw\")\n",
    "\n",
    "    # Create the driver\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--incognito')\n",
    "    chrome_options.add_argument(\"--mute-audio\")\n",
    "    if not WATCH_SCRAPE:\n",
    "        chrome_options.add_argument('--headless')\n",
    "        chrome_options.add_argument(\"--window-size=1440, 900\")\n",
    "\n",
    "    try:\n",
    "        web_driver = webdriver.Chrome(options=chrome_options, executable_path=EXECUTABLE_PATH)\n",
    "        logging.info('Chrome launched.')\n",
    "    except:\n",
    "        logging.critical(\n",
    "            'Chrome could not be launched. Check executable path and if Chromedriver supports the version of the browser.')\n",
    "\n",
    "##########################################\n",
    "# step 1: generate the list of urls for the videos that you want to scrape. we'll call this list urls\n",
    "# you can either pass it directly\n",
    "\n",
    "    # urls = [\n",
    "    #     \"https://www.youtube.com/watch?v=poRAut99bRk\"\n",
    "    #     , \"https://www.youtube.com/watch?v=A9mny08uI8s\"\n",
    "    #     , \"https://www.youtube.com/watch?v=AWyJ8vJQ6Vo\"\n",
    "    #     , \"https://www.youtube.com/watch?v=UpIcfw7Q14w\"\n",
    "    #     , \"https://www.youtube.com/watch?v=HgaHpLBGTRY\"\n",
    "    #     , \"https://www.youtube.com/watch?v=PAY56ovT4Qc\"\n",
    "    #     , \"https://www.youtube.com/watch?v=VQl2KVISh1w\"\n",
    "    # ]\n",
    "\n",
    "# or read it from an excel spreadsheet \n",
    "    # smaller_urls = pd.read_excel(\n",
    "    #     '/Users/jennifer.jin/OneDrive - Accenture Federal Services/Downloads/social_media/ES/filtered_df_orig_1293.xlsx')[\n",
    "    #     'url'].to_list()\n",
    "    # smaller_urls = strip_tag(smaller_urls)\n",
    "    # bigger_urls = pd.read_excel(\n",
    "    #     '/Users/jennifer.jin/OneDrive - Accenture Federal Services/Downloads/social_media/ES/filtered_df2.xlsx')[\n",
    "    #     'url'].to_list()\n",
    "    # urls = np.setdiff1d(bigger_urls, smaller_urls)\n",
    "\n",
    "    # urls = pd.read_excel(\n",
    "    #     '/Users/jennifer.jin/OneDrive - Accenture Federal Services/Downloads/social_media/ES/filtered_df.xlsx')[\n",
    "    #     'url'].to_list()\n",
    "\n",
    "    # urls_df = pd.read_excel(str(raw_data_dir / \"YOUTUBE_2020-08-06T16-29-59_METADATA.xlsx\"))\n",
    "    # urls_rescrape = urls_df[(urls_df.num_comments == 'None') & (urls_df.title != 'None')]\n",
    "    # urls = urls_rescrape['url'].to_list()\n",
    "    # yt_scrape_lots_of_videos(urls, False)\n",
    "\n",
    "    # or get it from the search results for a search query\n",
    "    \n",
    "    searchresultsdf, urls = get_vids(search_term, web_driver, num_vids)\n",
    "\n",
    "# or get it from the uploaded videos from a list of channels \n",
    "    # channels = [\"https://www.youtube.com/channel/UCMCgOm8GZkHp8zJ6l7_hIuA\"\n",
    "    # 'https://youtube.com/channel/UC0Wf8S7q9hDz8LbBDLVkiYw'\n",
    "    # , 'https://youtube.com/channel/UCPOYW7dOo_mqPP6-HkjJ2ng'\n",
    "    # , 'https://youtube.com/channel/UChGPLteUhl8SdM4ZuwZvyhQ'\n",
    "    # , 'https://youtube.com/channel/UCFdKC2cpE7mkBe2VAndcjqA'\n",
    "    # , 'https://youtube.com/user/Ironclaw007'\n",
    "    # , 'https://youtube.com/channel/UCp3qQc5YSUN-FbN3_qhZX1Q'\n",
    "    #         ]\n",
    "    # channel_df, urls = get_channel_vids(channels, web_driver, num_vids)\n",
    "\n",
    "##########################################\n",
    "# step 2: scrape the actual video metadata, transcript, comments, replies, etc from the list of urls\n",
    "    yt_scrape(urls, False, raw_data_dir, web_driver, max_num_scrolls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Add post-processing steps here\n",
    "\"\"\"\n",
    "\n",
    "# Clean up the environment\n",
    "web_driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
